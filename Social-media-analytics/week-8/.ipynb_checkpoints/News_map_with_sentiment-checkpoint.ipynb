{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "204846c0-5311-4c85-825d-4af5927cf959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in c:\\users\\umesh\\appdata\\roaming\\python\\python311\\site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\umesh\\appdata\\roaming\\python\\python311\\site-packages (4.9.2)\n",
      "Requirement already satisfied: folium in c:\\users\\umesh\\appdata\\roaming\\python\\python311\\site-packages (0.17.0)\n",
      "Requirement already satisfied: geopy in c:\\users\\umesh\\appdata\\roaming\\python\\python311\\site-packages (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\umesh\\appdata\\roaming\\python\\python311\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\umesh\\appdata\\roaming\\python\\python311\\site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\umesh\\appdata\\roaming\\python\\python311\\site-packages (from requests) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\umesh\\appdata\\roaming\\python\\python311\\site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\umesh\\appdata\\roaming\\python\\python311\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: branca>=0.6.0 in c:\\users\\umesh\\appdata\\roaming\\python\\python311\\site-packages (from folium) (0.7.2)\n",
      "Requirement already satisfied: jinja2>=2.9 in c:\\users\\umesh\\appdata\\roaming\\python\\python311\\site-packages (from folium) (3.1.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\umesh\\appdata\\roaming\\python\\python311\\site-packages (from folium) (1.24.4)\n",
      "Requirement already satisfied: xyzservices in c:\\programdata\\anaconda3\\lib\\site-packages (from folium) (2022.9.0)\n",
      "Requirement already satisfied: geographiclib<3,>=1.52 in c:\\users\\umesh\\appdata\\roaming\\python\\python311\\site-packages (from geopy) (2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\umesh\\appdata\\roaming\\python\\python311\\site-packages (from jinja2>=2.9->folium) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests beautifulsoup4 folium geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "959261ca-aa58-49c4-bbcc-bbf4fcdd728a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting textblob\n",
      "  Downloading textblob-0.18.0.post0-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: nltk>=3.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\umesh\\appdata\\roaming\\python\\python311\\site-packages (from nltk>=3.8->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\umesh\\appdata\\roaming\\python\\python311\\site-packages (from nltk>=3.8->textblob) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\umesh\\appdata\\roaming\\python\\python311\\site-packages (from nltk>=3.8->textblob) (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\umesh\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk>=3.8->textblob) (0.4.6)\n",
      "Downloading textblob-0.18.0.post0-py3-none-any.whl (626 kB)\n",
      "   ---------------------------------------- 0.0/626.3 kB ? eta -:--:--\n",
      "    --------------------------------------- 10.2/626.3 kB ? eta -:--:--\n",
      "   - ------------------------------------- 20.5/626.3 kB 682.7 kB/s eta 0:00:01\n",
      "   --- ----------------------------------- 61.4/626.3 kB 550.5 kB/s eta 0:00:02\n",
      "   --- ----------------------------------- 61.4/626.3 kB 550.5 kB/s eta 0:00:02\n",
      "   ------- ------------------------------ 122.9/626.3 kB 658.7 kB/s eta 0:00:01\n",
      "   ------- ------------------------------ 122.9/626.3 kB 658.7 kB/s eta 0:00:01\n",
      "   --------- ---------------------------- 163.8/626.3 kB 546.6 kB/s eta 0:00:01\n",
      "   --------- ---------------------------- 163.8/626.3 kB 546.6 kB/s eta 0:00:01\n",
      "   ----------- -------------------------- 194.6/626.3 kB 537.4 kB/s eta 0:00:01\n",
      "   ------------ ------------------------- 204.8/626.3 kB 498.9 kB/s eta 0:00:01\n",
      "   ------------ ------------------------- 204.8/626.3 kB 498.9 kB/s eta 0:00:01\n",
      "   -------------- ----------------------- 245.8/626.3 kB 471.0 kB/s eta 0:00:01\n",
      "   -------------- ----------------------- 245.8/626.3 kB 471.0 kB/s eta 0:00:01\n",
      "   --------------- ---------------------- 256.0/626.3 kB 403.5 kB/s eta 0:00:01\n",
      "   -------------------- ----------------- 337.9/626.3 kB 512.0 kB/s eta 0:00:01\n",
      "   ------------------------- ------------ 419.8/626.3 kB 582.6 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 471.0/626.3 kB 627.8 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 501.8/626.3 kB 642.5 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 501.8/626.3 kB 642.5 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 501.8/626.3 kB 642.5 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 501.8/626.3 kB 642.5 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 501.8/626.3 kB 642.5 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 501.8/626.3 kB 642.5 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 553.0/626.3 kB 518.9 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 553.0/626.3 kB 518.9 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 553.0/626.3 kB 518.9 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 553.0/626.3 kB 518.9 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 553.0/626.3 kB 518.9 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 553.0/626.3 kB 518.9 kB/s eta 0:00:01\n",
      "   ---------------------------------- --- 573.4/626.3 kB 424.1 kB/s eta 0:00:01\n",
      "   ---------------------------------- --- 573.4/626.3 kB 424.1 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 583.7/626.3 kB 403.5 kB/s eta 0:00:01\n",
      "   -------------------------------------- 626.3/626.3 kB 410.7 kB/s eta 0:00:00\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.18.0.post0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a583bd6a-3b0c-47a0-86ff-06eb5ae0167b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total articles from 2024-07-03 to 2024-07-03: 100\n",
      "color  lightgreen\n",
      "color  lightred\n",
      "color  lightgreen\n",
      "color  lightred\n",
      "color  lightblue\n",
      "color  lightgreen\n",
      "color  lightgreen\n",
      "color  lightgreen\n",
      "color  lightblue\n",
      "color  lightgreen\n",
      "color  lightgreen\n",
      "color  lightgreen\n",
      "color  lightgreen\n",
      "color  lightgreen\n",
      "color  lightgreen\n",
      "color  lightgreen\n",
      "color  lightred\n",
      "City counts:\n",
      "New York: 12 articles\n",
      "Washington: 4 articles\n",
      "San Antonio: 1 articles\n",
      "Total articles from 2024-07-17 to 2024-07-17: 100\n",
      "color  darkgreen\n",
      "color  darkgreen\n",
      "color  darkgreen\n",
      "color  darkblue\n",
      "color  darkblue\n",
      "color  darkred\n",
      "City counts:\n",
      "Washington: 1 articles\n",
      "WASHINGTON: 2 articles\n",
      "LAS VEGAS: 1 articles\n",
      "Philadelphia: 1 articles\n",
      "San Francisco: 1 articles\n",
      "Average sentiment 15 days ago: 0.10000148544266192\n",
      "Average sentiment yesterday: 0.08194444444444444\n",
      "Map generation complete. Check us_news_map.html for the map.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import folium\n",
    "from geopy.geocoders import Nominatim\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import random\n",
    "from textblob import TextBlob\n",
    "\n",
    "api_key = 'a'\n",
    "keyword = 'Donald Trump'\n",
    "base_url = 'https://newsapi.org/v2/everything'\n",
    "\n",
    "# Initialize geolocator\n",
    "geolocator = Nominatim(user_agent=\"news_geolocator\")\n",
    "\n",
    "# Initialize map centered on USA\n",
    "map = folium.Map(location=[37.0902, -95.7129], zoom_start=4)\n",
    "\n",
    "# Function to fetch news for a given date range and plot locations on map\n",
    "def fetch_and_plot_news(keyword, from_date, to_date, map, color_shade):\n",
    "    try:\n",
    "        params = {\n",
    "            'q': keyword,\n",
    "            'from': from_date,\n",
    "            'to': to_date,\n",
    "            'apiKey': api_key,\n",
    "            'pageSize': 100,  # Fetch up to 100 articles\n",
    "        }\n",
    "        response = requests.get(base_url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            news_data = response.json()\n",
    "            sentiments = []\n",
    "            if news_data.get('articles'):\n",
    "                city_counts = {}\n",
    "                print(f\"Total articles from {from_date} to {to_date}: {len(news_data['articles'])}\")\n",
    "                for article in news_data['articles']:\n",
    "                    city_name = extract_city_from_article(article)\n",
    "                    if city_name:\n",
    "                        if city_name in city_counts:\n",
    "                            city_counts[city_name] += 1\n",
    "                        else:\n",
    "                            city_counts[city_name] = 1\n",
    "                        \n",
    "                        location = geolocator.geocode(city_name)\n",
    "                        if location:\n",
    "                            # Add a small random offset to the latitude and longitude\n",
    "                            lat_offset = random.uniform(-0.9, 0.9)\n",
    "                            lon_offset = random.uniform(-0.9, 0.9)\n",
    "                            \n",
    "                            # Analyze sentiment of the article\n",
    "                            sentiment = analyze_sentiment(article['content'])\n",
    "                            sentiments.append(sentiment)\n",
    "                            if sentiment > 0:\n",
    "                                color = f'{color_shade}green'\n",
    "                            elif sentiment < 0:\n",
    "                                color = f'{color_shade}red'\n",
    "                            else:\n",
    "                                color = f'{color_shade}blue'\n",
    "\n",
    "                            # Limit title to 50 characters\n",
    "                            title = article['title'] if len(article['title']) <= 100 else article['title'][:97] + '...'\n",
    "                             # Format the publication date\n",
    "                            pub_date = datetime.strptime(article['publishedAt'], '%Y-%m-%dT%H:%M:%SZ').strftime('%A, %B %d, %Y')\n",
    "                            popup_content = f\"<b>{title}</b><br><i>Date: {pub_date}</i><br>Sentiment: {'Positive' if sentiment > 0 else 'Negative' if sentiment < 0 else 'Neutral'}\"\n",
    "                          \n",
    "                            folium.Marker(\n",
    "                                [location.latitude + lat_offset, location.longitude + lon_offset],\n",
    "                                popup=popup_content,\n",
    "                                icon=folium.Icon(color=color)\n",
    "                            ).add_to(map)\n",
    "                \n",
    "                # Print city counts\n",
    "                print(\"City counts:\")\n",
    "                for city, count in city_counts.items():\n",
    "                    print(f\"{city}: {count} articles\")\n",
    "                \n",
    "                # Check for cities with no data\n",
    "                all_cities = set(city_counts.keys())\n",
    "                cities_with_data = set(city_counts.keys())\n",
    "                cities_with_no_data = all_cities - cities_with_data\n",
    "                if cities_with_no_data:\n",
    "                    print(\"Cities with no data:\")\n",
    "                    for city in cities_with_no_data:\n",
    "                        print(city)\n",
    "            else:\n",
    "                print(f\"No articles found for the keyword from {from_date} to {to_date}.\")\n",
    "            \n",
    "            return sentiments\n",
    "        else:\n",
    "            print(f\"Error fetching news. Status code: {response.status_code}\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching or plotting news: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to extract city from article content\n",
    "def extract_city_from_article(article):\n",
    "    try:\n",
    "        if 'content' in article and article['content']:\n",
    "            text = article['content']\n",
    "            # Regular expression pattern to find city names\n",
    "            city_pattern = r'\\b(?:New York|Los Angeles|Chicago|Houston|Phoenix|Philadelphia|San Antonio|San Diego|Dallas|San Jose|Austin|Jacksonville|Fort Worth|Columbus|San Francisco|Charlotte|Indianapolis|Seattle|Denver|Washington|Boston|El Paso|Nashville|Detroit|Oklahoma City|Portland|Las Vegas|Memphis|Louisville|Baltimore)\\b'\n",
    "            \n",
    "            # Find all cities in the text using the pattern\n",
    "            cities_found = re.findall(city_pattern, text, flags=re.IGNORECASE)\n",
    "            \n",
    "            if cities_found:\n",
    "                # Return the first city found (you can modify as needed for your use case)\n",
    "                return cities_found[0]\n",
    "        \n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting city from article: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to analyze sentiment of the article content\n",
    "def analyze_sentiment(content):\n",
    "    blob = TextBlob(content)\n",
    "    return blob.sentiment.polarity\n",
    "\n",
    "# Calculate date ranges\n",
    "yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "fifteen_days_ago = (datetime.now() - timedelta(days=15)).strftime('%Y-%m-%d')\n",
    "\n",
    "# Fetch and plot news for 15 days ago (lighter colors)\n",
    "sentiments_15_days_ago = fetch_and_plot_news(keyword, fifteen_days_ago, fifteen_days_ago, map, 'light')\n",
    "\n",
    "# Fetch and plot news for yesterday (darker colors)\n",
    "sentiments_yesterday = fetch_and_plot_news(keyword, yesterday, yesterday, map, 'dark')\n",
    "\n",
    "# Compare average sentiments\n",
    "if sentiments_15_days_ago:\n",
    "    avg_sentiment_15_days_ago = sum(sentiments_15_days_ago) / len(sentiments_15_days_ago)\n",
    "else:\n",
    "    avg_sentiment_15_days_ago = None\n",
    "\n",
    "if sentiments_yesterday:\n",
    "    avg_sentiment_yesterday = sum(sentiments_yesterday) / len(sentiments_yesterday)\n",
    "else:\n",
    "    avg_sentiment_yesterday = None\n",
    "\n",
    "print(f\"Average sentiment 15 days ago: {avg_sentiment_15_days_ago}\")\n",
    "print(f\"Average sentiment yesterday: {avg_sentiment_yesterday}\")\n",
    "\n",
    "# Save the map to an HTML file\n",
    "map.save(\"us_news_map.html\")\n",
    "\n",
    "print(\"Map generation complete. Check us_news_map.html for the map.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
